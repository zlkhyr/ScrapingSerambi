{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd87704a-1400-45fa-b821-5fae93660840",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5843b8-08f2-43ae-9b1b-9d09bbdcb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import schedule as sc\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d414c0-42d2-4524-8fce-6f488ade4c61",
   "metadata": {},
   "source": [
    "## Fake UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f138bfee-8e1d-46b5-b538-5e783ce55f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FakeUserAgen = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6951413-536b-4889-8635-e3cbacbe6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass Error 403\n",
    "# FakeUserAgen = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824e07-e00f-4c1c-83c3-829a39dca752",
   "metadata": {},
   "source": [
    "## Scraping Serambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fbb571-74fc-4bcb-af42-2d94727559c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_serambi(tgl_awal, tgl_akhir, bln, thn):\n",
    "    titles = []\n",
    "    links = []\n",
    "    dates = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "    with tqdm(total=tgl_akhir-tgl_awal+1, desc=\"Processing\", unit=\"iter\") as pbar:\n",
    "        while tgl_awal <= tgl_akhir: #iterasi berita tiap hari \n",
    "            halaman = 1 #halaman def\n",
    "            ttl_hal = 1 #total halaman def\n",
    "            while halaman <= ttl_hal: #iterasi tiap halaman \n",
    "                #data retrieval\n",
    "                url = f'https://aceh.tribunnews.com/index-news/nanggroe?date={thn}-{bln}-{tgl_awal}&page={halaman}'\n",
    "                html = requests.get(url, headers=FakeUserAgen).text\n",
    "                bsobj = soup(html,'lxml')\n",
    "                data = bsobj.select('ul.lsi li.ptb15') #list berita\n",
    "                jmlh_hal = bsobj.find('div',{'class':'pt10 pb10'}) #jumlah berita \n",
    "                ttl_hal = math.ceil(int(jmlh_hal.find('b').text)/20) #jumalah halaman (jumlah berita/20 ceil) \n",
    "                for i in range(len(data)): #iterasi list berita\n",
    "                    try:\n",
    "                        html_cntn = requests.get(data[i].find('h3').find('a').get('href'), headers=FakeUserAgen).text\n",
    "                        bsobj_cntn = soup(html_cntn,'lxml')\n",
    "                        isi = bsobj_cntn.find('div',class_='side-article txt-article multi-fontsize editcontent')\n",
    "                        contents.append(isi.text)\n",
    "\n",
    "                        label = data[i].find('h4')\n",
    "                        labels.append(label.text if label is not None else 'null')\n",
    "                        titles.append(data[i].find('h3').find('a').get('title')) #get headlines\n",
    "                        links.append(data[i].find('h3').find('a').get('href')) #get links\n",
    "                        dates.append(f'{thn}-{bln}-{tgl_awal}') #get times\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "                    # print(i, end=' ')\n",
    "                    if i == 19:\n",
    "                        break #ganti halaman setelah berita ke-20\n",
    "                # print(f'|{halaman} done')\n",
    "                halaman += 1 #ganti halaman\n",
    "            pbar.update(1)\n",
    "            tgl_awal += 1 #ganti tanggal\n",
    "    # simpan kedalam dataframe\n",
    "    df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'headline': titles,\n",
    "    'link': links,\n",
    "    'content': contents,\n",
    "    'label' : labels\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'Serambi_{bln}-{thn}.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da7bd2-39a8-4e52-a9ba-08d291e1de69",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77677d8-244f-48eb-88a5-2f9945d786ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 30/30 [00:09<00:00,  3.03iter/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, headline, link, content, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gunakan method scraping_serambi disini ...\n",
    "scraping_serambi(1, 30, 6, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17220cd-ed48-4715-9b92-06f997896fbf",
   "metadata": {},
   "source": [
    "## Scraping otomatis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "971df727-b3ae-4fbb-b7f2-77956aa38e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Scraping():\n",
    "#     scraping_serambi(dt.now().date().day,dt.now().date().day,dt.now().date().month,dt.now().date().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fbdb529e-f87f-4082-8295-c524e12a534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.every().day.at('15:25').do(Scraping)\n",
    "# while True:\n",
    "#     sc.run_pending()\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0265147-d80c-4675-8dc7-8dc1c8a69159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
